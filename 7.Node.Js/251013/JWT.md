# 왜 우리는 JWT라는 것을 사용하게 되었는가?

JSON WEB TOKEN

JWT

근데, JWT 자체보다는
JWT를 왜 사용하게 되었는지 배경과 역사가 중요합니다.
=> JWT를 잘 다루기 위해서는!

## 태초에 우리는 정보를 전하고 싶었다.

네트워크 

내가 가진 정보를 누군가에게 전달하는 행위

- 누구는 글을 보여주고 싶었고
- 누구는 버튼을 누르면 결과를 응답하고 싶었고
- 누구는 로그인해서 개인 정보를 보고싶었다.

## 처음부터 요청과 응답이 있었던 건 아니다.

초창기에는

한 번 연결되면 데이터 쭉 주고 받자 ㅎㅎ

그런데 문제가 생긴다

- 사용자 수(요청 수)가 많아지면
- 연결을 계속 유지하는 구조에서는 서버가 감당할 자원이 폭증

=> 서버 과부하

그래서 요청과 응답이라는 하나의 과정을 만들었다

- 3-way-handshake
- 4-way-handshake

그리고 req, res

## 요청과 응답 구조

정보만 잘 전달되면 끝임?

- 누구는 한국어
- 누구는 중국어
- 누구는 영어

좀 통일하자....

- RESTful API
- Content-Type
- HTTP Method(GET, POST, PUT...)

## 권한! 그리고 매번 권한을 인증행하는 구조가 필요하다

- 쿠키
- 로컬
- 세션
- 
스토리지

## 근데? 사람들은 그 공간에 너무 많은 걸 담기 시작함

- 유저 ID...
- 권한 담고...
- 최근 본 게시글 몽땅 담고..

서버는 매번 해석하고 분석해야 하는디....

게다가 브라우저가 보낸 값은 기본적으로 이진 데이터임(바이너리 데이터)
(사실, base64라고 하는걸로 변환됨)

## 그래서? 생각하게 된다...

매번... 모든 값을 볼 필요가 있나?
나는 특정 속성 하나만 필요한디...
그리고 매번 DB 조회해야하나...

## 10. 예를 들어봅시다

어떤 사용자가 로그인해서 브라우저에 쿠키를 저장했다고 칩시다.

```json
{
  "user_id": "wnqudgus1234"
}
```

이 값이 서버에 잘 전달되고,  
그에 따라 사용자의 권한이 확인되었습니다.

그런데 누군가가 **의도적으로** 브라우저의 쿠키 값을 이렇게 바꿔버립니다.

```json
{
  "user_id": "wnqudgus1235"
}
```

> 겉으로 보기엔 한 글자 차이.  
> 하지만 내용은 완전히 다릅니다.  
> 다른 사용자의 권한을 탈취하려는 시도죠.

서버는 이걸 알아차리기 위해 **DB에 가서 비교**해야 합니다.  
그럼 트래픽이 늘어나고, 응답 시간이 늘어나고, 서버가 피곤해집니다.

## 그래서 생각했습니다 —

> “이 값을 인코딩해서 고정된 문자열로 만들자”

예를 들어 이 값을 단순한 숫자형 표현으로 바꾼다고 해봅시다.

```json
{
  "user_id": "wnqudgus1234"
}
```

→ `000010`  
(※ 실제 JWT 서명 구조와는 다르지만, 개념 설명을 위한 비유입니다)

그러면 누군가 이 값을 바꿔서  
`wnqudgus1235`가 되면?

→ `011101`  
(고작 한 글자를 바꿨는데도, 전체 값이 바뀌어 버립니다.)

즉, **변조는 바로 티가 납니다.**

```js
const token = req.authorizion.user_id;
jwt.verify(token);
```

이 값을 어디에 담느냐? => 쿠키나 로컬 세션 => 즉, 브라우저에 담는다.

또 하나의 핵심: 세상에 완벽한 암호화는 없다.
그리고 JWT의 주된 장점은 암호? 보안? 그렇게 대답하면 그러면 신기한 회사를 가는거에요.

자 이렇게 해서 좀 빌드업이 길었는데
그러면 어떻게 저런 외계어가 나왔는지 알아봐야겠져?

## 바이너리 데이터

0과 1로 이루어진 2진수
=> 컴퓨터가 이해할 수 있는 전기적 신호

### 문자에서 숫자로 => 표준의 시작 feat.아스키 코드

ASCII

아메리칸 스탠다드 코드 뽈 인포메이션 인터체인지
=> 영어를 지정된 코드로 며오학히 지정한다.

무슨 말인지..ㅠ

핵심: 우리가 정한 숫자나 문자를 컴퓨터가 이해하는 2진수로 변환해야 한다.

### 왜 문자를 숫자로 바꾸기로 했을까?

우리가 사용하는 문자들을 결국 컴퓨터가 이해하는 언어로 변환해야함.
왜? 그래야 로직 처리를 할 수 있기 때문에.

즉, A => 65 => 100010

이라는 숫자로 변환해야 컴퓨터가 알아머금

1960년대 초, IBM, DEC, xerox라는 회사들이 존재했는데, 각 회사마다
자신들만의 문자 부호 체계를 썼음.

| 회사  | ‘A’를 2진수로 변환하여 저장한 방식 |
| ----- | ---------------------------------- |
| IBM   | 01000001                           |
| DEC   | 00010001                           |
| Xerox | 10101010                           |

그래서, 이 상태로는 다른 컴퓨터끼리 데이터를 주고 받으면 문자가 깨짐 ㅠ
왜? A => 2진수로 바꾸었는데 서로 저장하는 형태가 다르니까 ㅠ

=> 모든 컴퓨터가 A를 같은 숫자로 65(10진수) 또는 01000001(2진수)로 저장하자!
=> 이것이 아스키코드(ASCII) => 세계 최초의 문자 표준!

### 왜 굳이 숫자로?

컴퓨터는 전류가 흐르는 상태(1)와 끊긴 상태(0)밖에 모른다.
즉, ‘A’라는 글자를 그대로 저장할 수 없으니,
그 글자에 **숫자**를 붙여서 저장해야 했다.

> “사람이 ‘A’를 입력하면, 기계는 01000001로 받아들이자.”

| 문자 | 10진수 | 16진수 | 2진수    |
| ---- | ------ | ------ | -------- |
| A    | 65     | 0x41   | 01000001 |
| B    | 66     | 0x42   | 01000010 |

이 표를 보면 알 수 있다.
‘A’는 10진수 65, 16진수 0x41, 2진수 01000001이다.
즉, 숫자 표기만 다를 뿐 같은 데이터를 바라보는 서로 다른 시점이다.

---

### 입력에서 출력까지

```
입력: 사람이 키보드로 A를 누름
→ 하드웨어는 A 키에 해당하는 스캔코드 신호를 전송
→ OS가 그 스캔코드를 ASCII 65로 변환
→ 메모리에 01000001로 저장
→ 화면은 코드 65를 보고 ‘A’로 렌더링
```

즉, 컴퓨터는 ‘A’를 모른다.
그저 “65 = A”라는 표준표를 참조할 뿐이다.

A = 65 
16진수로 나누면 

65 % 16 = 1(4)

0x41

그래서 이제 문제로 들어가보자...

그러면 American standard code for information아라매...

그러면... 다른 언어는 어떻게 표현함? 깨지는데?

=> 예시로 멀터! 파일 저장하는 외장 모듈
=> 기본적으로 아스키코드를 따름
=> 그래서 한글로 된 파일을 저장하면 깨지는거임!

## unicode(유니코드)

ASCII는 영어만 표현할 수 있었다.
하지만 세상에는 “가, あ, 愛, 🙂” 같은 수많은 문자가 있다.

각 나라마다, EUC-KR, Shift-JIS, GB2312처럼 자체 코드 체계를 만들자.

왜냐면, 같은 2진수라도 나라별로 해석이 달랐기 때문

| 나라마다 달랐던 문자 부호  | 결과         |
| ------------------------- | ------------ |
| EUC-KR                    | ‘가’ = B0 A1 |
| Shift-JIS                 | ‘가’ = 82 A0 |
| GB2312                    | ‘가’ = D6 D0 |

이 문제를 근본적으로 해결 및 통합하기 위해 나온 표가 Unicode다

### 유니코드의 약속

**세상의 모든 문자를 고유한 번호(code point)로 부여하자**

번호: 44032
A: 65
이모지: 128578

이런식으로 표현할 수 있게끔(위는 쉽게 예시를 든거임)

| 문자 | 코드 포인트 |
| ---- | ----------- |
| A    | U+0041      |
| 가   | U+AC00      |
| 🙂   | U+1F642     |

44032 => 1010 1100 000 000
65 => 0100 0001

문자는 크기가 제각각

어떤 문자는 2바이트 이떤 문자는 1바이트

이 저장 공간을 통일하고 싶음 ㅠ

## UTF(Unicode Transform Format)

UTF-8 => 영문은 1바이트, 나머지는 가변길이
UTF-16 => 대부분은 2바이트, 큰 문자는 4바이트
UTF-32 => 전부 4바이트로 통일

즉, 같은 코드 포인트도 인코딩 방식에 따라서 실제 저장값이 달라진다.

## 표준은 UTF-8이다.

가변길이 + 아스키 코드 호환성이라는 두 장점 때문에.

## 2진수, 10진수, 16진수...

숫자를 표현하는 여러가지 방식

=> 하나의 숫자 또는 문자라도 어떤 진법으로 읽느냐에 따라서 모양이 달라진다.
=> 즉, 진수마다 표현 방식의 차이이지, 실제 값 자체는 완전히 동일하다.

- **10진수 (Decimal)** : 우리가 일상적으로 쓰는 숫자 (0~9)
- **2진수 (Binary)** : 컴퓨터가 사용하는 0과 1의 조합
- **16진수 (Hexadecimal)** : 메모리나 색상 코드 등 개발에서 자주 쓰는 표기 (0~9, A~F)

숫자를 표현하는 여러 가지 방식

=> 하나의 숫자 또는 문자라도 어떤 진법으로 읽느냐에 따라서 모양이 달라진다.
=> 즉, 진수마다 표현 방식의 차이이지, 실제 값 자체는 완전히 동일하다.
=== 이런게 똑같다는게 